# -*- coding: utf-8 -*-
"""MODEL_EVALUATION.ipynb

Automatically generated by Colaboratory.

Written by:
Jairo El Yazidi Rios
Tea Shkurti

# Recommender Systems

Bootstrap is a statistical resampling technique that is used to estimate the quality of a model. It involves creating a large number of random samples with replacement from a dataset and using each of these samples to build and evaluate a model. By aggregating the results from each of these models, we can get a more accurate estimate of the model's performance.

In this tutorial, we will explore the steps involved in using bootstrap to estimate the quality of a model. We will use the Python programming language and several popular libraries for data manipulation and modeling, including numpy, pandas, and scikit-learn.

Libraries
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import MinMaxScaler

"""**Loading the data**

We are analyzing a bank dataset that includes demographic features of clients, such as their education level and whether they have a loan, as well as information about whether they accepted or declined a product offered to them during a contact with the bank. The dataset provides valuable insights into client behavior and can help the bank optimize its sales strategies.
"""

df = pd.read_csv('bank-additional-full.csv',sep=";")
df

"""Columns with real variables """

realColumns = [0,10,11,13,15,16,17,18,19]
df_real = df.iloc[:,realColumns]
df_real

"""Columns with categorical variables """

categoricalColumns = np.arange(0,21)[~np.isin(np.arange(0,21),np.array(realColumns))]

df_categorical = df.iloc[:,categoricalColumns]
df_categorical

"""Change categorical to dummies variable"""

pd.get_dummies(df_categorical.iloc[:, 0])

"""**Final dataset**"""

X = np.array(df_real) 
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

    
for i in range(df_categorical.shape[1]):
    X = np.c_[X,np.array(pd.get_dummies(df_categorical.iloc[:, i]))] 

print(X.shape)
X

"""Extracting the target variable"""

y = X[:,X.shape[1]-1]
X = X[:,0:(X.shape[1]-2)]

print(X.shape)
print(y.shape)

print(f"# of positive classes: {y.sum()}")
print(f"# of negatime classes: {y.shape[0]-y.sum()}")

"""Training and test split"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=123)
print(f"Training {X_train.shape} {y_train.shape}")
print(f"Test {X_test.shape} {y_test.shape}")

"""Classifing and predicting accuracy with KNN algorithm"""

model = KNeighborsClassifier(n_neighbors=100)
model.fit(X_train, y_train)
prediction = model.predict(X_test)
np.sum(prediction == y_test) / y_test.shape[0]

"""# Exercise

In this exercise, we will use bootstrapping to estimate the generalization error of the K-nearest neighbors (KNN) algorithm using various metrics including f1 score, accuracy, recall, and precision. Follow the instructions below and report the results:

For each metric, repeat the following process 100 times:

* Randomly select 10% of the data as the test set.
* Split the remaining 90% of the data into a training set and a validation set (in a 90/10 ratio).
* Fit the KNN model for different values of K and evaluate the validation performance according to the selected metric.
* Determine the best value of K based on the validation performance.
* Estimate the performance of the KNN model with the best K using the test set.

Finally, for each metric, calculate the average performance across the 100 executions for each metric.
"""

#Your code here
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
df = pd.read_csv('bank-additional-full.csv',sep=";")

realColumns = [0,10,11,13,15,16,17,18,19]
df_real = df.iloc[:,realColumns]
categoricalColumns = np.arange(0,21)[~np.isin(np.arange(0,21),np.array(realColumns))]
df_categorical = df.iloc[:,categoricalColumns]
X = np.array(df_real) 
scaler = MinMaxScaler()
X = scaler.fit_transform(X)
for i in range(df_categorical.shape[1]):
    X = np.c_[X,np.array(pd.get_dummies(df_categorical.iloc[:, i]))]

# Accuracy
scores = []
for i in range(100):
  data_train, data_test = train_test_split(X, test_size=0.1, random_state=i)
  X_test = data_test[:, :-1]
  y_test = data_test[:, -1]
  data_train, data_val = train_test_split(data_train, test_size=0.1, random_state=i) 
  X_train = data_train[:, :-1]
  y_train = data_train[:, -1]
  X_val = data_val[:, :-1]
  y_val = data_val[:, -1]
  neighbors = [1, 2, 3, 4, 5, 10, 18, 25]
  best_acc = 0
  K = 1
  for k in neighbors:
    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)
    prediction = model.predict(X_val)
    acc = accuracy_score(y_val, prediction)
    if acc > best_acc:
      best_acc = acc
      K = k
  print(K)
  model = KNeighborsClassifier(n_neighbors=K)
  model.fit(X_train, y_train)
  prediction = model.predict(X_test)
  acc = accuracy_score(y_test, prediction)
  scores.extend([acc])
  print(scores)
print(np.mean(scores))

# Recall
scores = []
for i in range(100):
  data_train, data_test = train_test_split(X, test_size=0.1, random_state=i)
  X_test = data_test[:, :-1]
  y_test = data_test[:, -1]
  data_train, data_val = train_test_split(data_train, test_size=0.1, random_state=i)
  X_train = data_train[:, :-1]
  y_train = data_train[:, -1]
  X_val = data_val[:, :-1]
  y_val = data_val[:, -1]
  neighbors = [1, 2, 3, 4, 5, 10, 18, 25]
  best_rec = 0
  K = 1
  for k in neighbors:
    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)
    prediction = model.predict(X_val)
    rec = recall_score(y_val, prediction)
    if rec > best_rec:
      best_rec = rec
      K = k
  print(K)
  model = KNeighborsClassifier(n_neighbors=K)
  model.fit(X_train, y_train)
  prediction = model.predict(X_test)
  rec = recall_score(y_test, prediction)
  scores.extend([rec])
  print(scores)
print(np.mean(scores))

# Precision
scores = []
for i in range(100):
  data_train, data_test = train_test_split(X, test_size=0.1, random_state=i)
  X_test = data_test[:, :-1]
  y_test = data_test[:, -1]
  data_train, data_val = train_test_split(data_train, test_size=0.1, random_state=i)
  X_train = data_train[:, :-1]
  y_train = data_train[:, -1]
  X_val = data_val[:, :-1]
  y_val = data_val[:, -1]
  neighbors = [1, 2, 3, 4, 5, 10, 18, 25]
  best_pre = 0
  K = 1
  for k in neighbors:
    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)
    prediction = model.predict(X_val)
    pre = precision_score(y_val, prediction)
    if pre > best_pre:
      best_pre = pre
      K = k
  print(K)
  model = KNeighborsClassifier(n_neighbors=K)
  model.fit(X_train, y_train)
  prediction = model.predict(X_test)
  pre = precision_score(y_test, prediction)
  scores.extend([pre])
  print(scores)
print(np.mean(scores))

# F1 score
scores = []
for i in range(100):
  data_train, data_test = train_test_split(X, test_size=0.1, random_state=i)
  X_test = data_test[:, :-1]
  y_test = data_test[:, -1]
  data_train, data_val = train_test_split(data_train, test_size=0.1, random_state=i)
  X_train = data_train[:, :-1]
  y_train = data_train[:, -1]
  X_val = data_val[:, :-1]
  y_val = data_val[:, -1]
  neighbors = [1, 2, 3, 4, 5, 10, 18, 25]
  best_f1 = 0
  K = 1
  for k in neighbors:
    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)
    prediction = model.predict(X_val)
    f1 = f1_score(y_val, prediction)
    if f1 > best_f1:
      best_f1 = f1
      K = k
  print(K)
  model = KNeighborsClassifier(n_neighbors=K)
  model.fit(X_train, y_train)
  prediction = model.predict(X_test)
  f1 = f1_score(y_test, prediction)
  scores.extend([f1])
  print(scores)
print(np.mean(scores))
